{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f24d8c5d-3e34-4781-b199-cfa73c89905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32113381-9ae1-4f5c-af4c-4a742eeebeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "                .master('local')\n",
    "                .appName('Interview_training')\n",
    "                .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78544df4-b41d-45f1-9422-ac9f4ef94ce3",
   "metadata": {},
   "source": [
    "### Q1 While ingesting customer data from an external source, you notice duplicate entries. How would you remove duplicates and retain only the latest entry based on a timestamp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "305a16a2-1a98-42d6-afa4-952b66f5ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n",
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-12-01|   100|\n",
      "|        101|2023-12-02|   150|\n",
      "|        102|2023-12-01|   200|\n",
      "|        102|2023-12-02|   250|\n",
      "+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(101, \"2023-12-01\", 100), [101, \"2023-12-02\", 150], \n",
    "        [102, \"2023-12-01\", 200], [102, \"2023-12-02\", 250]]\n",
    "schema = 'customer_id int, date string, amount int'\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn('date', col('date').cast(DateType()))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0ba5de-44b9-414b-99ef-eeb32ca5e1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-12-02|   150|\n",
      "|        102|2023-12-02|   250|\n",
      "+-----------+----------+------+\n",
      "\n",
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-12-02|   150|\n",
      "|        102|2023-12-02|   250|\n",
      "+-----------+----------+------+\n",
      "\n",
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-12-02|   150|\n",
      "|        102|2023-12-02|   250|\n",
      "+-----------+----------+------+\n",
      "\n",
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-12-02|   150|\n",
      "|        102|2023-12-02|   250|\n",
      "+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col('date').desc()).dropDuplicates(subset = ['customer_id']).show() #solution1\n",
    "df.sort(desc('date')).dropDuplicates(subset = ['customer_id']).show() #solution2\n",
    "df.orderBy(df['date'], ascending = [0]).dropDuplicates(subset = ['customer_id']).show() # solution3\n",
    "df.orderBy(df['date'], ascending = [False]).dropDuplicates(subset = ['customer_id']).show() # solution4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e37c63-ceef-4617-a21b-779090226d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----+\n",
      "|customer_id|      date|amount|rank|\n",
      "+-----------+----------+------+----+\n",
      "|        101|2023-12-02|   150|   1|\n",
      "|        102|2023-12-02|   250|   1|\n",
      "+-----------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#solution 5\n",
    "Window1 = Window.partitionBy(col('customer_id')).orderBy(col('date').desc())\n",
    "rn = row_number().over(Window1)\n",
    "df.withColumn('rank', rn).where('rank = 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d048f008-6159-457e-921f-67e06b564350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-12-02|   150|\n",
      "|        102|2023-12-02|   250|\n",
      "+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#solution 6\n",
    "df.createOrReplaceTempView('table1')\n",
    "\n",
    "spark.sql('''\n",
    "WITH cte1 as (\n",
    "    SELECT customer_id\n",
    "           ,date\n",
    "           ,amount\n",
    "           , ROW_NUMBER() OVER(PARTITION BY customer_id ORDER BY date DESC) as rank\n",
    "FROM table1\n",
    ")\n",
    "\n",
    "SELECT customer_id,\n",
    "        date,\n",
    "        amount FROM cte1 \n",
    "        where rank = 1 \n",
    "\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84209b4d-d4f7-4875-beb1-be757840d995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a73ab06-77eb-439a-a475-c17187730d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$Recycle.Bin', 'AMD', 'Documents and Settings', 'DumpStack.log.tmp', 'hadoop', 'hadoop-2.7.1', 'hiberfil.sys', 'Intel', 'MSI', 'mylog.log', 'OneDriveTemp', 'pagefile.sys', 'PerfLogs', 'Program Files', 'Program Files (x86)', 'ProgramData', 'Recovery', 'RHDSetup.log', 'Riot Games', 'swapfile.sys', 'System Volume Information', 'Users', 'Windows']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir('/')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f9df3-2280-4cab-92f7-3252bd373a10",
   "metadata": {},
   "source": [
    "### Q2 You need to calculate the total number of actions performed by users in a system. How would you calculate the top 5 most active users based on this information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3a6132a-e081-4cfc-8567-eabb15763a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|username|actions|\n",
      "+--------+-------+\n",
      "|   user1|      5|\n",
      "|   user2|      8|\n",
      "|   user3|      2|\n",
      "|   user2|      9|\n",
      "|   user5|      5|\n",
      "|   user1|      8|\n",
      "|   user2|      2|\n",
      "|   user3|      9|\n",
      "|   user4|      5|\n",
      "|   user6|      8|\n",
      "|   user7|      2|\n",
      "|   user3|      8|\n",
      "+--------+-------+\n",
      "\n",
      "+--------+-----------+\n",
      "|username|avg_actions|\n",
      "+--------+-----------+\n",
      "|   user3|         19|\n",
      "|   user2|         19|\n",
      "|   user1|         13|\n",
      "|   user6|          8|\n",
      "|   user5|          5|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data  = [(\"user1\", 5), (\"user2\", 8), (\"user3\", 2), (\"user2\", 9),\n",
    "        (\"user5\", 5), (\"user1\", 8), (\"user2\", 2), (\"user3\", 9),\n",
    "        (\"user4\", 5), (\"user6\", 8), (\"user7\", 2), (\"user3\", 8)]\n",
    "\n",
    "schema = 'username string, actions int'\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "df.groupBy('username').agg(sum(col('actions')).alias('avg_actions')).orderBy(col('avg_actions').desc()).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c17b8e-873c-4289-a034-1e60c1ed5d36",
   "metadata": {},
   "source": [
    "### Q3 While processing sales transaction data, you nedd to identify the most recent transaction for each customer. How would you approach this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88d34c8a-839a-477d-ab9e-8fd0f83d568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(101, \"2023-12-01\", 100), (101, \"2023-12-02\", 150), \n",
    "        (102, \"2023-12-01\", 200), (102, \"2023-12-02\", 250),\n",
    "       (103, \"2024-12-20\", 400), (103, \"2022-01-02\", 150), \n",
    "        (102, \"2025-02-10\", 200), (105, \"2025-01-02\", 250)]\n",
    "schema = 'customer_id string, date string, amount int'\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn('date', col('date').cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd317879-43b6-458a-9c62-cde22d8be8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-12-02|   150|\n",
      "|        102|2025-02-10|   200|\n",
      "|        103|2024-12-20|   400|\n",
      "|        105|2025-01-02|   250|\n",
      "+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(col('customer_id')).orderBy(col('date').desc())\n",
    "df.withColumn('rank', dense_rank().over(window_spec)).where('rank = 1').drop('rank').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b4cf0d-295d-4644-b389-924e3c537ce3",
   "metadata": {},
   "source": [
    "### Q4 You need to identify customers who haven't made any purchases in the last 30 days. How would you filter such customers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f77c42db-11bb-45ca-9350-c60f34adc019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"cust1\", \"2025-03-01\"),(\"cust2\", \"2025-03-11\"), (\"cust3\", \"2025-02-01\"),  (\"cust1\", \"2025-01-21\")]\n",
    "schema = 'customer string, date string'\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df = df.withColumn('date', col('date').cast(DateType()))\n",
    "df.printSchema()\n",
    "\n",
    "df=df.withColumn('gap', date_diff(current_date(),  col('date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35d55647-a6c0-4c7d-bd55-3d9a8decca70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---+\n",
      "|customer|      date|gap|\n",
      "+--------+----------+---+\n",
      "|   cust3|2025-02-01| 40|\n",
      "|   cust1|2025-01-21| 51|\n",
      "+--------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where('gap > 30').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cda043-ca95-4ba7-b6f9-b342df4ac1cc",
   "metadata": {},
   "source": [
    "### Q5 While analyzing customer reviews, you need to identify the most frequently used words in the feedback. How would you implement this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35996d60-d83e-46d5-8673-8286308127cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|customer_id|            feedback|\n",
      "+-----------+--------------------+\n",
      "|  customer1|The product is great|\n",
      "|  customer2|Great product, fa...|\n",
      "|  customer3|Not bad, could be...|\n",
      "+-----------+--------------------+\n",
      "\n",
      "+--------+---------+\n",
      "|    word|wordcount|\n",
      "+--------+---------+\n",
      "|   great|        2|\n",
      "| product|        2|\n",
      "|   could|        1|\n",
      "|     not|        1|\n",
      "|      be|        1|\n",
      "|    fast|        1|\n",
      "|      is|        1|\n",
      "|     bad|        1|\n",
      "|     the|        1|\n",
      "|delivery|        1|\n",
      "|  better|        1|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"customer1\", \"The product is great\"), (\"customer2\", \"Great product, fast delivery\"), (\"customer3\", \"Not bad, could be better\")]\n",
    "columns = [\"customer_id\", \"feedback\"]\n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()\n",
    "\n",
    "df=df.withColumn('feedback', lower('feedback'))\\\n",
    "    .withColumn(\"feedback\", regexp_replace(col(\"feedback\"), \",\", \"\"))\\\n",
    "    .withColumn('word', explode(split(col('feedback'), ' ')))\n",
    "\n",
    "df.groupBy('word').agg(count(col('word')).alias('wordcount')).orderBy(desc('wordcount')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbaef24-e971-455a-9d8f-3e2abfd66481",
   "metadata": {},
   "source": [
    "### Q6 You need to calculate the cumulative sum of sales over time for each product. How would you approach this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98aaa660-731b-485b-a4b1-5e9a2e34e1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|product_id|      date|sales|\n",
      "+----------+----------+-----+\n",
      "|  product1|2023-12-01|  100|\n",
      "|  product2|2023-12-02|  200|\n",
      "|  product1|2023-12-03|  150|\n",
      "|  product2|2023-12-04|  250|\n",
      "|  product3|2023-12-01|  500|\n",
      "|  product4|2023-12-02|  200|\n",
      "|  product1|2024-10-03|  250|\n",
      "|  product2|2023-03-04|  550|\n",
      "+----------+----------+-----+\n",
      "\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- sales: long (nullable = true)\n",
      "\n",
      "+----------+----------+-----+-------+\n",
      "|product_id|      date|sales|cum_sum|\n",
      "+----------+----------+-----+-------+\n",
      "|  product1|2023-12-01|  100|    100|\n",
      "|  product1|2023-12-03|  150|    250|\n",
      "|  product1|2024-10-03|  250|    500|\n",
      "|  product2|2023-03-04|  550|    550|\n",
      "|  product2|2023-12-02|  200|    750|\n",
      "|  product2|2023-12-04|  250|   1000|\n",
      "|  product3|2023-12-01|  500|    500|\n",
      "|  product4|2023-12-02|  200|    200|\n",
      "+----------+----------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"product1\", \"2023-12-01\", 100), (\"product2\", \"2023-12-02\", 200),\n",
    "        (\"product1\", \"2023-12-03\", 150), (\"product2\", \"2023-12-04\", 250),\n",
    "        (\"product3\", \"2023-12-01\", 500), (\"product4\", \"2023-12-02\", 200),\n",
    "        (\"product1\", \"2024-10-03\", 250), (\"product2\", \"2023-03-04\", 550)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "window_spec = Window.partitionBy(col('product_id')).orderBy('date')\n",
    "df.withColumn('cum_sum', sum('sales').over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eefd61-5ce8-4afc-9ffe-1827de5ad580",
   "metadata": {},
   "source": [
    "### Q7 While analyzing sales data, you need to find the product with the highest sales for each month. How would you accomplish this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51ff1586-1d24-4726-8697-8a93f3a91dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+----+-----+\n",
      "|product_id|      date|sales|year|month|\n",
      "+----------+----------+-----+----+-----+\n",
      "|  product1|2023-12-01|  100|2023|   12|\n",
      "|  product2|2023-12-02|  200|2023|   12|\n",
      "|  product1|2023-12-03|  150|2023|   12|\n",
      "|  product2|2023-12-04|  250|2023|   12|\n",
      "|  product3|2023-12-01|  500|2023|   12|\n",
      "|  product4|2023-12-02|  200|2023|   12|\n",
      "|  product1|2024-10-03|  250|2024|   10|\n",
      "|  product2|2023-03-04|  550|2023|    3|\n",
      "+----------+----------+-----+----+-----+\n",
      "\n",
      "+----------+----------+-----+----+-----+---+\n",
      "|product_id|      date|sales|year|month| dr|\n",
      "+----------+----------+-----+----+-----+---+\n",
      "|  product2|2023-03-04|  550|2023|    3|  1|\n",
      "|  product3|2023-12-01|  500|2023|   12|  1|\n",
      "|  product1|2024-10-03|  250|2024|   10|  1|\n",
      "+----------+----------+-----+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn('year', year('date')).withColumn('month', month('date'))\n",
    "df.show()\n",
    "window_spec = Window.partitionBy('year', 'month').orderBy(desc('sales'))\n",
    "df = df.withColumn('dr', dense_rank().over(window_spec)).where(\"dr = 1 \")  \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c74724-e5db-4dfe-b9de-2f034cbba4b0",
   "metadata": {},
   "source": [
    "### Q8 While processing sales data, you need to classify each transaction as either 'High' or 'Low' based on its amount. How would you achieve this using a when condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f3ff7f8-d6b0-4d6a-ad94-5f31906bcaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|sales|\n",
      "+----------+-----+\n",
      "|  product1|  100|\n",
      "|  product2|  300|\n",
      "|  product3|   50|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+--------------+\n",
      "|product_id|sales|classification|\n",
      "+----------+-----+--------------+\n",
      "|  product1|  100|        Medium|\n",
      "|  product2|  300|          High|\n",
      "|  product3|   50|           Low|\n",
      "+----------+-----+--------------+\n",
      "\n",
      "+----------+-----+--------------+\n",
      "|product_id|sales|classification|\n",
      "+----------+-----+--------------+\n",
      "|  product1|  100|        Medium|\n",
      "|  product2|  300|          High|\n",
      "|  product3|   50|           Low|\n",
      "+----------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"product1\", 100) , (\"product2\", 300), (\"product3\", 50)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()\n",
    "\n",
    "df.withColumn(\"classification\", when(col('sales') >100, 'High')\n",
    "                                .when(col('sales') > 50, 'Medium')\n",
    "                                .otherwise('Low')).show()\n",
    "\n",
    "df.createOrReplaceTempView('table1')\n",
    "spark.sql('''\n",
    "    Select product_id, sales,\n",
    "            CASE\n",
    "                WHEN sales > 100 THEN 'High'\n",
    "                WHEN sales > 50 THEN 'Medium'\n",
    "            ELSE 'Low' \n",
    "            END as classification\n",
    "    FROM table1\n",
    "''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
