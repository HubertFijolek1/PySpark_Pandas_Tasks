{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f24d8c5d-3e34-4781-b199-cfa73c89905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32113381-9ae1-4f5c-af4c-4a742eeebeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "                .master('local')\n",
    "                .appName('Interview_training')\n",
    "                .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78544df4-b41d-45f1-9422-ac9f4ef94ce3",
   "metadata": {},
   "source": [
    "### Q1 While ingesting customer data from an external source, you notice duplicate entries. How would you remove duplicates and retain only the latest entry based on a timestamp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "305a16a2-1a98-42d6-afa4-952b66f5ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-12-01|   100|\n",
      "|        101|2023-12-02|   150|\n",
      "|        102|2023-12-01|   200|\n",
      "|        102|2023-12-02|   250|\n",
      "+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(101, \"2023-12-01\", 100), [101, \"2023-12-02\", 150], \n",
    "        [102, \"2023-12-01\", 200], [102, \"2023-12-02\", 250]]\n",
    "schema = 'customer_id int, date string, amount int'\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn('date', col('date').cast(DateType()))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ea0ba5de-44b9-414b-99ef-eeb32ca5e1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-12-02|   150|\n",
      "|        102|2023-12-02|   250|\n",
      "+-----------+----------+------+\n",
      "\n",
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-12-02|   150|\n",
      "|        102|2023-12-02|   250|\n",
      "+-----------+----------+------+\n",
      "\n",
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-12-02|   150|\n",
      "|        102|2023-12-02|   250|\n",
      "+-----------+----------+------+\n",
      "\n",
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-12-02|   150|\n",
      "|        102|2023-12-02|   250|\n",
      "+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col('date').desc()).dropDuplicates(subset = ['customer_id']).show() #solution1\n",
    "df.sort(desc('date')).dropDuplicates(subset = ['customer_id']).show() #solution2\n",
    "df.orderBy(df['date'], ascending = [0]).dropDuplicates(subset = ['customer_id']).show() # solution3\n",
    "df.orderBy(df['date'], ascending = [False]).dropDuplicates(subset = ['customer_id']).show() # solution4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04e37c63-ceef-4617-a21b-779090226d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----+\n",
      "|customer_id|      date|amount|rank|\n",
      "+-----------+----------+------+----+\n",
      "|        101|2023-12-02|   150|   1|\n",
      "|        102|2023-12-02|   250|   1|\n",
      "+-----------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#solution 5\n",
    "Window1 = Window.partitionBy(col('customer_id')).orderBy(col('date').desc())\n",
    "rn = row_number().over(Window1)\n",
    "df.withColumn('rank', rn).where('rank = 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d048f008-6159-457e-921f-67e06b564350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-12-02|   150|\n",
      "|        102|2023-12-02|   250|\n",
      "+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#solution 6\n",
    "df.createOrReplaceTempView('table1')\n",
    "\n",
    "spark.sql('''\n",
    "WITH cte1 as (\n",
    "    SELECT customer_id\n",
    "           ,date\n",
    "           ,amount\n",
    "           , ROW_NUMBER() OVER(PARTITION BY customer_id ORDER BY date DESC) as rank\n",
    "FROM table1\n",
    ")\n",
    "\n",
    "SELECT customer_id,\n",
    "        date,\n",
    "        amount FROM cte1 \n",
    "        where rank = 1 \n",
    "\n",
    "''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
